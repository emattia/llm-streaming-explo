{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import cohere\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(os.environ['COHERE_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello World!\"\n",
    "MODEL = \"command\"\n",
    "COST_PER_1M_TOKENS = {'input': 1.0, 'output': 2.0} # https://cohere.com/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_params = dict(message=message,  model=MODEL,  temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.chat(**api_params)\n",
    "answer = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I assist you today? This is Coral, an AI-assistant chatbot, welcoming you to engage in a conversation with me. If you have any questions or need assistance with a particular task, feel free to let me know, and I'll do my best to provide you with helpful and thorough responses. \n",
      "\n",
      "It's great to chat with you, have a fantastic day!\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input cost: $5.4e-05 \n",
      "Output cost: $0.00016\n"
     ]
    }
   ],
   "source": [
    "input_cost = response.meta['billed_units']['input_tokens'] * COST_PER_1M_TOKENS['input'] / 1000000\n",
    "output_cost = response.meta['billed_units']['output_tokens'] * COST_PER_1M_TOKENS['output'] / 1000000\n",
    "print(f\"Input cost: ${input_cost} \\nOutput cost: ${output_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    {\"user_name\": \"User\", \"text\": \"Hey!\"},\n",
    "\t{\"user_name\": \"Chatbot\", \"text\": \"Hey! How can I help you today?\"}\n",
    "]\n",
    "message = \"What do LLMs do?\"\n",
    "api_params = dict(chat_history=chat_history, message=message, model=MODEL, temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = co.chat(**api_params)\n",
    "answer = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are artificial intelligence tools that have been trained on massive amounts of text data and can generate human-like language in response to prompts. Their ability to comprehend and create natural language makes them valuable for a variety of tasks, including:\n",
      "\n",
      "1. **Language Translation:** LLMs can translate text or speech from one language to another with increasing accuracy and fluency. They can handle complex linguistic tasks and capture the nuances of different languages, aiding in cross-language communication.\n",
      "\n",
      "2. **Text Completion:** LLMs can generate coherent and contextually relevant text when given a prompt or a starting point. This is useful for generating descriptive language, completing partial sentences or paragraphs, or even storytelling.\n",
      "\n",
      "3. **Question Answering**: Given a question, LLMs can retrieve the relevant information from the knowledge database they have been trained on and provide answers. They can handle complex questions and deliver informative responses, although the accuracy depends on the quality of the training data and the specific LLM architecture.\n",
      "\n",
      "4. **Predictive Texting**: LLMs can suggest the next word or phrase based on the input provided, enabling faster and more efficient writing. This can be particularly helpful when drafting emails, documents, or even coding.\n",
      "\n",
      "5. **Content Generation**: Beyond text completion, LLMs can generate original content on a wide range of topics. From news articles to creative writing, LLMs can produce coherent passages, but it is important to note that the quality and originality of this content depend on the specific model and its training.\n",
      "\n",
      "6. **Assistant Roles**: LLMs power virtual assistants that can understand and respond to natural language queries from users. These assistants can handle a variety of tasks, from scheduling appointments to answering factual questions, providing information, and controlling smart home devices.\n",
      "\n",
      "7. **Learning and Education**: LLMs can also assist in learning and education by generating practice questions, providing personalized feedback, and adapting to individual learners' needs. \n",
      "\n",
      "It's important to note that LLMs are continually evolving and improving, and their applications continue to expand with time. \n",
      "\n",
      "Would you like to know more about any specific application of Large Language Models?\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are artificial intelligence tools that have been trained on massive amounts of text data and can generate human-like language in response to prompts. They are designed to perform a wide range of natural language processing tasks, including:\n",
      "\n",
      "1. **Language Translation**: LLMs can translate text or speech from one language to another with increasing accuracy and fluency. \n",
      "\n",
      "2. **Text Completion**: LLMs can generate coherent and contextually appropriate responses to partial or truncated sentences, helping to create natural-sounding dialogue.\n",
      "\n",
      "3. **Question Answering**: Given a question, LLMs can retrieve the relevant information from the knowledge base and provide a response. They are able to understand the context and the relationships between words to provide more accurate answers.\n",
      "\n",
      "4. **Story Generation**: LLMs can create unique and coherent stories when provided with a prompt or a set of guidelines. They can also summarize long texts while retaining the key information.\n",
      "\n",
      "5. **Predictive Texting**: LLMs can suggest the next word or phrase based on the input provided, helping to speed up writing processes and reducing typos.\n",
      "\n",
      "6. **Content Creation**: LLMs can generate texts on a wide array of topics, from writing emails to composing marketing copy to scripting dialogues in fictional settings.\n",
      "\n",
      "7. **Error Correction**: LLMs can identify and correct grammatical errors, spelling mistakes, and misused words in written texts.\n",
      "\n",
      "8. **Named Entity Recognition**: LLMs can identify and categorize named entities such as person names, organization names, and locations in text.\n",
      "\n",
      "9. **Language Identification**: LLMs can identify the language of a given text, which is useful in multilingual environments.\n",
      "\n",
      "10. **Sentiment Analysis**: LLMs can analyze text to determine the sentiment or emotional tone behind it, which is valuable in understanding public opinion on social media or in customer reviews.\n",
      "\n",
      "These are just a few examples of what LLMs can do, and new applications are being developed continuously. \n",
      "\n",
      "It's important to note that while LLMs have advanced rapidly in recent years, they may still make mistakes or produce biased or inaccurate outputs under certain circumstances. Prompt engineering (the art of creating prompts that yield the best results) and ongoing research are working to address these issues and improve LLM performance. \n",
      "\n",
      "Would you like me to go into more detail on any of the above applications?"
     ]
    }
   ],
   "source": [
    "api_params['stream'] = True\n",
    "for response in co.chat(**api_params):\n",
    "    if response.event_type == 'stream-start':\n",
    "        continue\n",
    "    elif response.event_type == 'stream-end':\n",
    "        break\n",
    "    print(response.text, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello to you as well! How can I assist you today? If you would like, we can have a conversation about anything you'd like to discuss. Alternatively, if you have any questions or need assistance with a particular task, feel free to let me know, and I'll do my best to help you out. \n",
      "\n",
      "Would you like me to suggest some conversation topics?"
     ]
    }
   ],
   "source": [
    "api_params['stream'] = True\n",
    "full_response = \"\"\n",
    "for response in co.chat(**api_params):\n",
    "    if response.event_type == 'stream-start':\n",
    "        continue\n",
    "    elif response.event_type == 'stream-end':\n",
    "        break\n",
    "    elif response.event_type == 'text-generation':\n",
    "        print(response.text, end='', flush=True)\n",
    "        full_response += response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in response: 424\n",
      "\n",
      "Output cost: $0.000848\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "# https://huggingface.co/Cohere/Command-nightly\n",
    "tokenizer = Tokenizer.from_pretrained(\"Cohere/command-nightly\")\n",
    "enc = tokenizer.encode(full_response)\n",
    "number_tokens = len(enc.ids)\n",
    "output_cost = number_tokens * COST_PER_1M_TOKENS['output'] / 1000000\n",
    "print(\"Number of tokens in response:\", number_tokens)\n",
    "print(f\"\\nOutput cost: ${output_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But how do we know that this amount of tokens is what Cohere tells us they are billing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello World!\"\n",
    "preamble = \"\"\n",
    "api_params = dict(message=message, model=MODEL, temperature=0.9, preamble_override=preamble, chat_history=[])\n",
    "response = co.chat(**api_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Responses: does the open-source tokenizer match what the Cohere API reports?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.meta['billed_units']['output_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in response: 63\n"
     ]
    }
   ],
   "source": [
    "enc = tokenizer.encode(response.text)\n",
    "number_tokens = len(enc.ids)\n",
    "print(\"Number of tokens in response:\", number_tokens)\n",
    "\n",
    "# Observation, this consistently returns 2 tokens more than the number of tokens in the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.meta['billed_units']['input_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_msgs = [\n",
    "    \"Hello World! blah blah blah blah\",\n",
    "    \"What do LLMs do? 27l. 91. 26.\",\n",
    "    \"How do I use the API? {storage: [1, 2, 3], compute: [ec2, azure-vm, compute-engine}\",\n",
    "    \"What is the meaning of life?\",\n",
    "    \"What is the best movie of all time?\",\n",
    "]\n",
    "\n",
    "data = {\n",
    "    'msg': [],\n",
    "    'response': [],\n",
    "    'estimated_input_tokens': [],\n",
    "    'estimated_output_tokens': [],\n",
    "    'actual_input_tokens': [],\n",
    "    'actual_output_tokens': []\n",
    "}\n",
    "\n",
    "INPUT_BUFFER_TOKENS = 49\n",
    "OUTPUT_BUFFER_TOKENS = -2\n",
    "\n",
    "for msg in sample_msgs:\n",
    "    \n",
    "    api_params = dict(message=msg, model=MODEL, temperature=0.9, preamble_override=preamble, chat_history=[])\n",
    "    response = co.chat(**api_params)\n",
    "\n",
    "    enc = tokenizer.encode(msg)\n",
    "    est_in=len(enc.ids) + INPUT_BUFFER_TOKENS\n",
    "    est_out=len(tokenizer.encode(response.text).ids) + OUTPUT_BUFFER_TOKENS\n",
    "    \n",
    "    data['msg'].append(msg)\n",
    "    data['response'].append(response.text)\n",
    "    data['estimated_input_tokens'].append(est_in)\n",
    "    data['estimated_output_tokens'].append(est_out)\n",
    "    data['actual_input_tokens'].append(response.meta['billed_units']['input_tokens'])\n",
    "    data['actual_output_tokens'].append(response.meta['billed_units']['output_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg</th>\n",
       "      <th>response</th>\n",
       "      <th>estimated_input_tokens</th>\n",
       "      <th>estimated_output_tokens</th>\n",
       "      <th>actual_input_tokens</th>\n",
       "      <th>actual_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello World! blah blah blah blah</td>\n",
       "      <td>Hello to you as well! It's great to hear from ...</td>\n",
       "      <td>58</td>\n",
       "      <td>112</td>\n",
       "      <td>58</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do LLMs do? 27l. 91. 26.</td>\n",
       "      <td>I'm sorry, I am unable to respond to your requ...</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "      <td>64</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I use the API? {storage: [1, 2, 3], com...</td>\n",
       "      <td>To use the API, you need to perform the follow...</td>\n",
       "      <td>82</td>\n",
       "      <td>452</td>\n",
       "      <td>82</td>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the meaning of life?</td>\n",
       "      <td>The meaning of life is a philosophical questio...</td>\n",
       "      <td>58</td>\n",
       "      <td>212</td>\n",
       "      <td>58</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the best movie of all time?</td>\n",
       "      <td>Determining the \"best movie of all time\" is su...</td>\n",
       "      <td>60</td>\n",
       "      <td>541</td>\n",
       "      <td>60</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 msg  \\\n",
       "0                   Hello World! blah blah blah blah   \n",
       "1                      What do LLMs do? 27l. 91. 26.   \n",
       "2  How do I use the API? {storage: [1, 2, 3], com...   \n",
       "3                       What is the meaning of life?   \n",
       "4                What is the best movie of all time?   \n",
       "\n",
       "                                            response  estimated_input_tokens  \\\n",
       "0  Hello to you as well! It's great to hear from ...                      58   \n",
       "1  I'm sorry, I am unable to respond to your requ...                      64   \n",
       "2  To use the API, you need to perform the follow...                      82   \n",
       "3  The meaning of life is a philosophical questio...                      58   \n",
       "4  Determining the \"best movie of all time\" is su...                      60   \n",
       "\n",
       "   estimated_output_tokens  actual_input_tokens  actual_output_tokens  \n",
       "0                      112                   58                   112  \n",
       "1                       68                   64                    68  \n",
       "2                      452                   82                   452  \n",
       "3                      212                   58                   212  \n",
       "4                      541                   60                   541  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts: does Cohere count `chat_history` against the billed tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_msgs_w_history = [\n",
    "    (\n",
    "        \"Hello World! blah blah blah blah\", \n",
    "        [\n",
    "            {\"role\": \"USER\", \"message\": \"Who discovered gravity?\"},\n",
    "            {\"role\": \"CHATBOT\", \"message\": \"Isaac Newton\"}\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"What do LLMs do? 27l. 91. 26.\", \n",
    "        [\n",
    "            {\"role\": \"USER\", \"message\": \"Who contributes to the international space station?\"},\n",
    "            {\"role\": \"CHATBOT\", \"message\": \"The United States, Russia, Japan, Canada, and the European Space Agency\"}\n",
    "        ]\n",
    "    ),\n",
    "]\n",
    "\n",
    "data = {\n",
    "    'msg': [],\n",
    "    'response': [],\n",
    "    'estimated_input_tokens': [],\n",
    "    'estimated_output_tokens': [],\n",
    "    'actual_input_tokens': [],\n",
    "    'actual_output_tokens': []\n",
    "}\n",
    "\n",
    "def get_n_tokens(string):\n",
    "    return len(tokenizer.encode(string).ids)\n",
    "\n",
    "INPUT_BUFFER_TOKENS = 49\n",
    "CHAT_HISTORY_ADJUSTMENT = -4\n",
    "OUTPUT_BUFFER_TOKENS = -2\n",
    "\n",
    "for msg, history in sample_msgs_w_history:\n",
    "    \n",
    "    api_params = dict(message=msg, model=MODEL, temperature=0.9, preamble_override=preamble, chat_history=history)\n",
    "    response = co.chat(**api_params)\n",
    "\n",
    "    est_in = get_n_tokens(msg) + sum([get_n_tokens(h['message']) for h in history]) + INPUT_BUFFER_TOKENS + CHAT_HISTORY_ADJUSTMENT\n",
    "    est_out = len(tokenizer.encode(response.text).ids) + OUTPUT_BUFFER_TOKENS\n",
    "    \n",
    "    data['msg'].append(msg)\n",
    "    data['response'].append(response.text)\n",
    "    data['estimated_input_tokens'].append(est_in)\n",
    "    data['estimated_output_tokens'].append(est_out)\n",
    "    data['actual_input_tokens'].append(response.meta['billed_units']['input_tokens'])\n",
    "    data['actual_output_tokens'].append(response.meta['billed_units']['output_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg</th>\n",
       "      <th>response</th>\n",
       "      <th>estimated_input_tokens</th>\n",
       "      <th>estimated_output_tokens</th>\n",
       "      <th>actual_input_tokens</th>\n",
       "      <th>actual_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hello World! blah blah blah blah</td>\n",
       "      <td>Hello to you too! \\n\\nWould you like me to bla...</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do LLMs do? 27l. 91. 26.</td>\n",
       "      <td>I'm sorry, I am unable to respond to this requ...</td>\n",
       "      <td>87</td>\n",
       "      <td>98</td>\n",
       "      <td>87</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                msg  \\\n",
       "0  Hello World! blah blah blah blah   \n",
       "1     What do LLMs do? 27l. 91. 26.   \n",
       "\n",
       "                                            response  estimated_input_tokens  \\\n",
       "0  Hello to you too! \\n\\nWould you like me to bla...                      65   \n",
       "1  I'm sorry, I am unable to respond to this requ...                      87   \n",
       "\n",
       "   estimated_output_tokens  actual_input_tokens  actual_output_tokens  \n",
       "0                       19                   65                    19  \n",
       "1                       98                   87                    98  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-typewriter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
